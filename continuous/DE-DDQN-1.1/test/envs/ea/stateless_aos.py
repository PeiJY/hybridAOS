import random
import numpy as np
import copy
from math import sqrt, log

def random_select(Prob):
    return np.random.choice(np.arange(len(Prob)), p=Prob)

class stateless_AOS:
    def __init__(self, ope_num, CA='FI', OS = 'PM', wu=0.9,wl=0.1, P_max = 0.5, Alpha = 0.5, Beta = 0.5, Theta = 0.01, Lambda = 0.5):
        self.K = ope_num
        self.CA = CA
        self.OS = OS
        self.P_max = P_max
        self.P_min = (1 - P_max) / (self.K - 1)
        
        self.Alpha = Alpha # discount factor in PM and AP
        self.Beta = Beta #  step length of prob update in AP
        self.Theta = Theta # setp length of m update in DMAB
        self.Lambda = Lambda # threshold of HP test in DMAB

        # self.CA_candidates = ['FI','FI-normalized','FI_extreme']
        self.CA_candidates = ['FI','success','FIR']
        self.OS_candidates = ['PM','AP','DMAB','random','round']

        self.select_best_one = False
        if OS in ['DMAB','round']:
            self.select_best_one = True

        

        self.restart_records()
        self.global_n = [0 for _ in range(self.K)] # total number of using of each operator, for log record

        self.w= wu # for joint stateless and state-based
        self.wu=wu
        self.wl=wl

    def weight_update(self, improved):
        # print(self.w)
        if improved:
            self.w = (self.w + self.wu)/2
            return self.w
        else:
            self.w = (self.w + self.wl)/2
            return self.w
    
    def weight_update2(self, chosen_AOS, improved):
        if chosen_AOS == 0: # SL used
            if improved:
                self.w = self.w + (self.wu-self.w)/self.K
                return self.w
            else:
                self.w = self.w + (self.wl-self.w)/self.K
                return self.w
        else:  # SB used
            if improved:
                self.w = self.w + (self.wl-self.w)/self.K
                return self.w
            else:
                self.w = self.w + (self.wu-self.w)/2
                return self.w

    # def weight_update3(self, is_stateless_used, improved)

    def joint_prob(self,Psb,Psl):
        # print(Psb)
        # print(Psl)
        Prob = Psb
        for i in range(len(Psb)):
            Prob[i] = (1-self.w)*Psb[i] + self.w*Psl[i]
        # print(Prob)
        return Prob

    def joint_select(self,Sb_action):
        choose = random_select([self.w,1-self.w])
        if choose == 0:
            return choose, self.select_ope()
        else:
            return choose, Sb_action

    


    # simple use of AOS
    def step(self, origin_dist, new_dist, used_ope = -1):
        """ simple use of stateless, calculting reward, updating records and selection probability.
            operator are used to apply once, generating one offspring

        Args:
            origin_dist (_type_): distance of origin solution
            new_dist (_type_): distrance of new solution generated by last operator using
            used_ope (int, optional): las used operator. Defaults to -1.

        Returns:
            int: index of operator selected for next using
        """        
        if self.OS == 'DMAB' or self.OS == 'round':
            select_best_one = True
        else:
            select_best_one = False
        self.update(origin_dist, new_dist, used_ope)
        selected_ope_index = self.select_ope(select_best_one)
        return selected_ope_index

    def update(self, origin_dist, new_dist, used_ope = -1):
        """ calculate reward and update selection probability

        Args:
            origin_dist (_type_): distance of origin solution
            new_dist (_type_): distrance of new solution generated by last operator using
            used_ope (int, optional): las used operator. Defaults to -1.

        Returns:
            int[]: selection probability for the next operator using
        """        
        if used_ope == -1:
            used_ope = self.selected_ope
        reward = self.calculate_reward(origin_dist, new_dist)
        self.update_select_probs(reward, used_ope)
        return self.P




    def restart_records(self, ope_to_use = -1):

        self.R = [0.0 for _ in range(self.K)] # reward of each operator
        self.Q = [0.0 for _ in range(self.K)] # cumurated rewards of each operator
        self.P = [(1/self.K) for _ in range(self.K)] # selection probability of each operator
        self.n = [0 for _ in range(self.K)] # number of using of each operator
        self.M = [0.0 for _ in range(self.K)] # for HP test in DMAB
        self.m = [0.0 for _ in range(self.K)] # for HP test in DMAB
        
        if ope_to_use == -1:
            self.selected_ope = random.randint(0,self.K-1)
        else:
            self.selected_ope = ope_to_use
    
    def select_ope(self, select_best_one = -1):
        if select_best_one == -1:
            select_best_one = self.select_best_one
        selcted_index = 0
        if select_best_one: # select the one with highest P
            selcted_index = self.P.index(max(self.P))
        else: # select based on the probability as P
            selcted_index = np.random.choice(np.arange(self.K), p=self.P)
        return selcted_index


    def calculate_reward(self, origin_dist, new_dist, smaller_better=True):
        if smaller_better:
            if self.CA == 'FI':
                return max(0,origin_dist-new_dist) 
            elif self.CA == 'FIR':
                return max(0,(origin_dist-new_dist)/origin_dist) 
            elif self.CA == 'success':
                if origin_dist > new_dist:
                    return 1
                else: 
                    return 0
            else:
                print('calculate_reward, CA does not exist: ',self.CA)
                raise ValueError("stateless AOS: CA does not exist")
        else:
            if self.CA == 'FI':
                return max(0,new_dist>origin_dist) 
            elif self.CA == 'FIR':
                return max(0,(new_dist-origin_dist)/origin_dist) 
            elif self.CA == 'success':
                if origin_dist < new_dist:
                    return 1
                else: 
                    return 0
            else:
                print('calculate_reward, CA does not exist: ',self.CA)
                raise ValueError("stateless AOS: CA does not exist")

    def update_select_probs_multi(self,rewards, num_use):
        """ calculate and update selection probability after multiple operator using of different operators

        Args:
            rewards (number[]): the rewards of operators' using
            num_use (int[]): the number of times of using of each operator
        Raises:
            ValueError: OS not exist

        Returns:
            float[]: operators' selection probabilities
        """    

        if self.OS == 'PM':
            for i in range(self.K):
                self.Q[i] = self.Q[i] + self.Alpha * (rewards[i] - self.Q[i])
            sum_Q = sum(self.Q)
            if sum_Q != 0:
                for i in range(self.K):
                    self.P[i] = self.P_min + (1 - self.K * self.P_min) * (self.Q[i] / sum_Q)
            for i in range(self.K):
                self.global_n[i] += num_use[i]

        if self.OS == 'AP':
            for i in range(self.K):
                self.Q[i] = self.Q[i] + self.Alpha * (rewards[i] - self.Q[i])
            sum_Q = sum(self.Q)
            max_Q = max(self.Q)
            max_Q_index = self.Q.index(max_Q)
            for i in range(self.K):
                if i == max_Q_index:
                    self.P[i] = self.P[i] + self.Beta * (self.P_max - self.P[i])
                else:
                    self.P[i] = self.P[i] + self.Beta * (self.P_min - self.P[i])
            for i in range(self.K):
                self.global_n[i] += num_use[i]
        
        if self.OS == 'DMAB':
            print('DMAB not support for multi operator using')
            raise ValueError("stateless AOS: DMAB not support for multi operator using")

        if self.OS == 'random':
            for i in range(self.K):
                self.P[i] = 1.0/self.K
        
        if self.OS == 'round':
            print('round not support for multi operator using')
            raise ValueError("stateless AOS: round not support for multi operator using")

        if self.OS not in self.OS_candidates:
            print("OS does not exist")
            raise ValueError("stateless AOS: OS does not exist")

        return self.P

    def update_select_probs(self, reward, ope):
        """ calculate and update selection probability after one operator used once

        Args:
            reward (number): the reward of last operator using
            ope (int): last used operator

        Raises:
            ValueError: OS not exist

        Returns:
            float[]: operators' selection probabilities
        """        
        if self.OS == 'PM':
            self.Q[ope] = self.Q[ope] + self.Alpha * (reward - self.Q[ope])
            sum_Q = sum(self.Q)
            if sum_Q != 0:
                for i in range(self.K):
                    self.P[i] = self.P_min + (1 - self.K * self.P_min) * (self.Q[i] / sum_Q)
            self.global_n[ope] += 1

        if self.OS == 'AP':
            self.Q[ope] = self.Q[ope] + self.Alpha * (reward - self.Q[ope])
            sum_Q = sum(self.Q)
            max_Q = max(self.Q)
            max_Q_index = self.Q.index(max_Q)
            for i in range(self.K):
                if i == max_Q_index:
                    self.P[i] = self.P[i] + self.Beta * (self.P_max - self.P[i])
                else:
                    self.P[i] = self.P[i] + self.Beta * (self.P_min - self.P[i])
            self.global_n[ope] += 1
        
        if self.OS == 'DMAB':
            self.Q[ope] = (self.n[ope] * self.Q[ope] + reward) / (self.n[ope] + 1) 
            self.n[ope] += 1
            self.global_n[ope] += 1
            self.m[ope] = self.m[ope] + (self.Q[ope] - reward + self.Theta)
            self.M[ope] = max(self.M[ope],self.m[ope])
            if self.M[ope] - self.m[ope] > self.Lambda:
                self.restart_records()
                # print('restart')
            for i in range(self.K):
                if self.n[i] == 0:
                    self.P[i] = float('inf')
                else:
                    self.P[i] = self.Q[i] + sqrt(2 * log(sum(self.n,2)) / self.n[i])

        if self.OS == 'random':
            for i in range(self.K):
                self.P[i] = 1.0/self.K
        
        if self.OS == 'round':
            self.global_n[ope] += 1
            self.P = [0.0 for _ in range(self.K)]
            temp_n = sum(self.global_n)
            self.P[temp_n % self.K] = 1.0


        if self.OS not in self.OS_candidates:
            print("OS does not exist")
            raise ValueError("stateless AOS: OS does not exist")

        return self.P
        